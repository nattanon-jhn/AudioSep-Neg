import random
import torch
import torch.nn as nn
import torchaudio
from transformers import RobertaTokenizer

from models.CLAP.open_clip import create_model
from models.CLAP.training.data import get_audio_features


class CLAP_Encoder(nn.Module):
    """
    CLAP encoder à¸ªà¸³à¸«à¸£à¸±à¸š AudioSep
    - à¹ƒà¸™ __init__ à¸ˆà¸°à¹„à¸¡à¹ˆà¹‚à¸«à¸¥à¸” pretrained checkpoint à¹‚à¸”à¸¢à¸•à¸£à¸‡ (à¹€à¸žà¸·à¹ˆà¸­à¹€à¸¥à¸µà¹ˆà¸¢à¸‡ PyTorch 2.6 pickle error)
    - à¹ƒà¸Šà¹‰ pretrained="" à¹€à¸žà¸·à¹ˆà¸­à¸à¸±à¸™à¹„à¸¡à¹ˆà¹ƒà¸«à¹‰ create_model() à¸žà¸±à¸‡à¹€à¸žà¸£à¸²à¸° pretrained=None
    - à¹ƒà¸Šà¹‰ load_audio_pretrained() à¹‚à¸«à¸¥à¸” weights à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸ safe_globals apply à¹à¸¥à¹‰à¸§
    """

    def __init__(
        self,
        pretrained_path='checkpoint/music_speech_audioset_epoch_15_esc_89.98.pt',
        sampling_rate=32000,
        amodel="HTSAT-base",
    ):
        super().__init__()

        self.device = "cpu"
        self.precision = "fp32"
        self.amodel = amodel  
        self.tmodel = "roberta"
        self.enable_fusion = False
        self.fusion_type = "aff_2d"

        self.pretrained_path = pretrained_path
        self.sampling_rate = sampling_rate

        # -------------------------------------------------------
        # â— FIX à¸ªà¸³à¸„à¸±à¸: à¸«à¹‰à¸²à¸¡à¹ƒà¸Šà¹‰ pretrained=None
        # create_model() à¸¡à¸µ pretrained.lower() â†’ None à¸žà¸±à¸‡à¸—à¸±à¸™à¸—à¸µ
        # -------------------------------------------------------
        self.model, self.model_cfg = create_model(
            amodel_name=self.amodel,
            tmodel_name=self.tmodel,
            pretrained="",              # à¹ƒà¸Šà¹‰ string à¸§à¹ˆà¸²à¸‡à¹à¸—à¸™ None
            precision=self.precision,
            device=self.device,
            enable_fusion=self.enable_fusion,
            fusion_type=self.fusion_type,
        )

        self.tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

        # Freeze parameters
        for p in self.model.parameters():
            p.requires_grad = False

        self.model.eval()
        self.encoder_type = "CLAP"

    # -------------------------------------------------------
    # à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¹‚à¸«à¸¥à¸” pretrained CLAP (à¹‚à¸«à¸¥à¸”à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸ safe_globals apply à¹à¸¥à¹‰à¸§)
    # -------------------------------------------------------
    def load_audio_pretrained(self, ckpt_path=None):
        ckpt_path = ckpt_path or self.pretrained_path

        print(f"ðŸ”„ Loading CLAP pretrained weights: {ckpt_path}")
        try:
            ckpt = torch.load(ckpt_path, map_location="cpu")
            state_dict = ckpt.get("model", ckpt)
            missing, unexpected = self.model.load_state_dict(state_dict, strict=False)
            print("   âœ” Loaded CLAP weights")
            print("   Missing keys:", missing)
            print("   Unexpected keys:", unexpected)
        except Exception as e:
            print(f"   âš ï¸ Failed to load CLAP pretrained weights: {e}")
            print("   à¸ˆà¸°à¹ƒà¸Šà¹‰ random weights à¹à¸—à¸™ (à¸„à¸¸à¸“à¸ à¸²à¸žà¸ˆà¸°à¸”à¹‰à¸­à¸¢à¸¥à¸‡)")

    # -------------------------------------------------------
    # Utility
    # -------------------------------------------------------

    def batch_to_list(self, batch):
        return [batch[i] for i in range(batch.size(0))]

    # -------------------------------------------------------
    # Audio embedding
    # -------------------------------------------------------
    def _get_audio_embed(self, batch):
        # batch: [B, samples]
        with torch.no_grad():
            assert self.sampling_rate == 32000, "Only support 32000Hz input"

            # Resample to 48k for HTSAT
            batch = torchaudio.functional.resample(
                batch,
                orig_freq=self.sampling_rate,
                new_freq=48000,
            )

            audio_dicts = []
            for waveform in self.batch_to_list(batch):
                audio_dict = {}
                audio_dict = get_audio_features(
                    audio_dict,
                    waveform,
                    480000,  # target len for HTSAT
                    data_truncating="fusion",
                    data_filling="repeatpad",
                    audio_cfg=self.model_cfg["audio_cfg"],
                )
                audio_dicts.append(audio_dict)

            embed = self.model.get_audio_embedding(audio_dicts)
            return embed.detach()

    # -------------------------------------------------------
    # Text embedding
    # -------------------------------------------------------
    def _get_text_embed(self, batch):
        double_batch = False

        if len(batch) == 1:
            batch = batch * 2
            double_batch = True

        with torch.no_grad():
            text_data = self.tokenizer(
                batch,
                padding="max_length",
                truncation=True,
                max_length=512,
                return_tensors="pt",
            )
            # convert batch dims so model.get_text_embedding() à¹ƒà¸Šà¹‰à¹„à¸”à¹‰
            text_data = {k: v for k, v in text_data.items()}
            embed = self.model.get_text_embedding(text_data)

        if double_batch:
            embed = embed[0].unsqueeze(0)

        return embed.detach()

    # -------------------------------------------------------
    # Public API: get query embedding
    # -------------------------------------------------------
    def get_query_embed(self, modality, audio=None, text=None, use_text_ratio=0.5, device=None):
        if modality == "audio":
            embed = self._get_audio_embed(audio)
        elif modality == "text":
            embed = self._get_text_embed(text)
        elif modality == "hybird":
            if random.random() > use_text_ratio:
                embed = self._get_audio_embed(audio)
            else:
                embed = self._get_text_embed(text)
        else:
            raise NotImplementedError("Unknown modality type")

        return embed.float()
